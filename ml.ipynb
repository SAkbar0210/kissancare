{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load dataset (Iris for example)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset (Iris for example)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the k-NN classifier with k=5 neighbors\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9867149758454107\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.98      1.00      0.99       295\n",
      "           sci.med       0.99      0.99      0.99       305\n",
      "talk.politics.misc       1.00      0.97      0.98       228\n",
      "\n",
      "          accuracy                           0.99       828\n",
      "         macro avg       0.99      0.99      0.99       828\n",
      "      weighted avg       0.99      0.99      0.99       828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset (subset of newsgroups dataset for English text classification)\n",
    "data = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.med', 'talk.politics.misc'], shuffle=True, random_state=42)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text data into numerical features using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Create the NaÃ¯ve Bayes classifier (MultinomialNB suited for text classification)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.2946\n",
      "Epoch 100, Loss: 0.0368\n",
      "Epoch 200, Loss: 0.0163\n",
      "Epoch 300, Loss: 0.0103\n",
      "Epoch 400, Loss: 0.0075\n",
      "Epoch 500, Loss: 0.0059\n",
      "Epoch 600, Loss: 0.0048\n",
      "Epoch 700, Loss: 0.0041\n",
      "Epoch 800, Loss: 0.0035\n",
      "Epoch 900, Loss: 0.0031\n",
      "hi => Class: 0, Probabilities: [[1. 0. 0.]]\n",
      "no => Class: 2, Probabilities: [[0. 0. 1.]]\n",
      "yes => Class: 1, Probabilities: [[0. 1. 0.]]\n",
      "hello => Class: 0, Probabilities: [[1. 0. 0.]]\n",
      "hey => Class: 0, Probabilities: [[0.99 0.   0.01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "words = ['hi', 'hello', 'yes', 'no']\n",
    "labels = [0, 0, 1, 2]  # 0: Greeting, 1: Affirmative, 2: Negative\n",
    "\n",
    "# One-hot encoding of characters (simple and fixed size)\n",
    "def one_hot_encode(word, max_len=5):\n",
    "    word = word.lower()\n",
    "    base = np.zeros((max_len, 26))\n",
    "    for i, char in enumerate(word[:max_len]):\n",
    "        if char.isalpha():\n",
    "            base[i][ord(char) - ord('a')] = 1\n",
    "    return base.flatten()\n",
    "\n",
    "X = np.array([one_hot_encode(w) for w in words])\n",
    "y = np.array([[1, 0, 0],  # hi\n",
    "              [1, 0, 0],  # hello\n",
    "              [0, 1, 0],  # yes\n",
    "              [0, 0, 1]]) # no\n",
    "\n",
    "# Neural Network Parameters\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 3\n",
    "lr = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Weight initialization\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Activation Functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    y_pred = softmax(z2)\n",
    "\n",
    "    # Loss (Cross Entropy)\n",
    "    loss = -np.sum(y * np.log(y_pred + 1e-9)) / len(X)\n",
    "\n",
    "    # Backward pass\n",
    "    dz2 = y_pred - y\n",
    "    dW2 = a1.T @ dz2\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    dz1 = dz2 @ W2.T * sigmoid_deriv(a1)\n",
    "    dW1 = X.T @ dz1\n",
    "    db1 = np.sum(dz1, axis=0)\n",
    "\n",
    "    # Update weights\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Prediction\n",
    "def predict(word):\n",
    "    vec = one_hot_encode(word).reshape(1, -1)\n",
    "    a1 = sigmoid(vec @ W1 + b1)\n",
    "    output = softmax(a1 @ W2 + b2)\n",
    "    return np.argmax(output), output\n",
    "\n",
    "# Test\n",
    "test_words = [\"hi\", \"no\", \"yes\", \"hello\", \"hey\"]\n",
    "for word in test_words:\n",
    "    pred, prob = predict(word)\n",
    "    print(f\"{word} => Class: {pred}, Probabilities: {prob.round(2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
